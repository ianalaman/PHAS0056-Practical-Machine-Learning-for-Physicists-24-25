{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning for Physicists\n",
    "# Coursework D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will be trying to predict a system using incomplete information. We will set up the equations of motions for a simple double pendulum (or should that be a double simple pendulum. Then we will see if a machine learning technique can predict the future position of the lower mass, using only the lower mass positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinematics of the double pendulum\n",
    "Let's specify our problem in terms of the following, with the origin at the pivot point of the top pendulum. This is just background for the machine learning tasks at the bottom of the notebook.\n",
    "\n",
    "#### Positions\n",
    "$$x_1 = L_1 \\sin \\theta_1$$\n",
    "$$y_1 = -L_1 \\cos \\theta_1$$\n",
    "$$x_2 = x_1 + L_2 \\sin \\theta_2$$\n",
    "$$y_2 = y_1 - L_2 \\cos \\theta_2$$\n",
    "\n",
    "#### Velocities\n",
    "$$\\dot{x}_1 = \\dot{\\theta_1} L_1 \\cos \\theta_1$$\n",
    "$$\\dot{y_1} =  \\dot{\\theta_1} L_1 \\sin \\theta_1$$\n",
    "$$\\dot{x_2} = \\dot{x_1} + \\dot{\\theta_2} L_2 \\cos \\theta_2$$\n",
    "$$\\dot{y_2} = \\dot{y_1} + \\dot{\\theta_2} L_2 \\sin \\theta_2$$\n",
    "\n",
    "\n",
    "#### Accelerations\n",
    "\n",
    "$$\\ddot{x}_1 = -\\dot{\\theta_1}^2 L_1 \\sin \\theta_1 + \\ddot{\\theta_1} L_1 \\cos \\theta_1$$\n",
    "$$\\ddot{y_1} =  \\dot{\\theta_1}^2 L_1 \\cos \\theta_1 + \\ddot{\\theta_1} L_1 \\sin \\theta_1$$\n",
    "$$\\ddot{x_2} = \\ddot{x_1} - \\dot{\\theta_2}^2 L_2 \\sin \\theta_2 + \\ddot{\\theta_2} L_2 \\cos \\theta_2$$\n",
    "$$\\ddot{y_2} = \\ddot{y_1} + \\dot{\\theta_2}^2 L_2 \\cos \\theta_2 + \\ddot{\\theta_2} L_2 \\sin \\theta_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energies\n",
    "Let $v_1^2 = \\dot{x_1}^2 +\\dot{y_1}^2$ and $v_2^2 = \\dot{x_2}^2 +\\dot{y_2}^2$ then the kinetic energies $T_1$ and $T_2$ are\n",
    "$$ T_1 = \\frac{1}{2}m_1 v_1^2 = \\frac{1}{2}m_1 L_1^2 \\dot{\\theta_1}^2 $$\n",
    "$$ T_2 = \\frac{1}{2}m_2 v_2^2 = \\frac{1}{2}m_2 \\left( L_1^2 \\dot{\\theta_1}^2 + L_2^2 \\dot{\\theta_2}^2 + 2L_1 L_2 \\cos(\\theta_1-\\theta_2) \\dot{\\theta_1} \\dot{\\theta_2} \\right) $$\n",
    "\n",
    "The potential enrgies are\n",
    "$$V_1 = m_1 g y_1 = - m_1 g L_1 \\cos \\theta_1$$\n",
    "$$V_2 = m_2 g y_2 = -m_2 g ( L_1 \\cos \\theta_1 + L_2 \\cos \\theta_2)$$\n",
    "\n",
    "#### Langrangian\n",
    "Now we form the Lagrangian $L=T-V=T_1+T_2 -V_1 -V_2$ and use the Euler-Lagrange equations:\n",
    "$$\\frac{\\partial L}{\\partial \\theta_1} = \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta_1}}$$\n",
    "$$\\frac{\\partial L}{\\partial \\theta_2} = \\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta_2}}$$\n",
    "\n",
    "Applying these gives\n",
    "$$-(m_1+m_2) g L_1 \\sin \\theta_1 = (m_1+m_2) L_1^2 \\ddot{\\theta_1} + m_2 L_1 L_2 \\sin(\\theta_1-\\theta_2) \\dot{\\theta_2}^2 +  m_2 L_1 L_2 \\cos(\\theta_1-\\theta_2) \\ddot{\\theta_2} $$\n",
    "and\n",
    "$$ -m_2 g L_2 \\sin \\theta_2 = m_2 L_2 \\ddot{\\theta_2} + m_2 L_1 L_2 \\cos(\\theta_1-\\theta_2) \\ddot{\\theta_1} + m_2 L_1 L_2 \\sin(\\theta_1-\\theta_2) \\dot{\\theta_1}^2 $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equations of motions\n",
    "$$ \\omega_1 = \\dot{\\theta_1}$$  \n",
    "\n",
    "$$ \\omega_2 = \\dot{\\theta_2}$$ \n",
    "$$ \\ddot\\theta_1 = \\frac{1}{L_1\\xi}\\left[L_1m_2\\cos(\\theta_1-\\theta_2)\\sin(\\theta_1-\\theta_2)\\omega_1^2 + L_2m_2\\sin(\\theta_1-\\theta_2)\\omega_2^2 - m_2g\\cos(\\theta_1-\\theta_2)\\sin(\\theta_2) + (m_1+m_2)g\\sin(\\theta_1) \\right] $$\n",
    "$$ \\ddot\\theta_2 = \\frac{1}{L_2\\xi}\\left[L_2m_2\\cos(\\theta_1-\\theta_2)\\sin(\\theta_1-\\theta_2)\\omega_2^2 + L_1(m_1+m_2)\\sin(\\theta_1-\\theta_2)\\omega_1^2+(m_1+m_2)g\\sin(\\theta_1)\\cos(\\theta_1-\\theta_2) - (m_1+m_2)g\\sin(\\theta_2) \\right] $$\n",
    "where \n",
    "$$\\xi \\equiv \\cos^2(\\theta_1-\\theta_2)m_2-m_1-m_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "import matplotlib.style #Some style nonsense\n",
    "import matplotlib as mpl #Some more style nonsense\n",
    "\n",
    "#Set default figure size\n",
    "#mpl.rcParams['figure.figsize'] = [12.0, 8.0] #Inches... of course it is inches\n",
    "mpl.rcParams[\"legend.frameon\"] = False\n",
    "mpl.rcParams['figure.dpi']=200 # dots per inch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhs(t, z, L1, L2, m1, m2, g):\n",
    "    \"\"\"\n",
    "    Returns the right-hand side of the ordinary differential equation describing the double pendulem\n",
    "    \"\"\"\n",
    "    theta1, w1, theta2, w2 = z    #The four components\n",
    "    cos12 = np.cos(theta1 - theta2)\n",
    "    sin12 = np.sin(theta1 - theta2)\n",
    "    sin1 = np.sin(theta1)\n",
    "    sin2 = np.sin(theta2)\n",
    "    xi = cos12**2*m2 - m1 - m2\n",
    "    w1dot = ( L1*m2*cos12*sin12*w1**2 + L2*m2*sin12*w2**2\n",
    "            - m2*g*cos12*sin2      + (m1 + m2)*g*sin1)/(L1*xi)\n",
    "    w2dot = -( L2*m2*cos12*sin12*w2**2 + L1*(m1 + m2)*sin12*w1**2\n",
    "            + (m1 + m2)*g*sin1*cos12  - (m1 + m2)*g*sin2 )/(L2*xi)\n",
    "    return w1, w1dot, w2, w2dot   #Return the w's and the wdot's\n",
    "\n",
    "\n",
    "def to_cartesian(theta1, w1, theta2, w2, L1, L2):\n",
    "    \"\"\" Transforms theta and omega to cartesian coordinates\n",
    "    and velocities x1, y1, x2, y2, vx1, vy1, vx2, vy2\n",
    "    \"\"\"\n",
    "    x1 = L1 * np.sin(theta1)\n",
    "    y1 = -L1 * np.cos(theta1)\n",
    "    x2 = x1 + L2 * np.sin(theta2)\n",
    "    y2 = y1 - L2 * np.cos(theta2)\n",
    "    vx1 = L1*np.cos(theta1)*w1\n",
    "    vy1 = L1*np.sin(theta1)*w1\n",
    "    vx2 = vx1 + L2*np.cos(theta2)*w2\n",
    "    vy2 = vy1 + L2*np.sin(theta2)*w2\n",
    "    return x1, y1, x2, y2, vx1, vy1, vx2, vy2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the initial conditions. Here we have lengths and masses\n",
    "L1, L2 = 1., 1.\n",
    "m1, m2 = 3., 1.\n",
    "g = 9.81     # [m/s^2]. Gravitational acceleration\n",
    "\n",
    "#Starting angles\n",
    "z0=[np.pi/4,0,np.pi/4,0]\n",
    "#z0=[0.1,0,0.1,0]\n",
    "\n",
    "#Time ranges\n",
    "tmax, dt = 50, 0.1\n",
    "t = np.arange(0, tmax+dt, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve initial value problem\n",
    "ret = solve_ivp(rhs, (0,tmax), z0, t_eval=t, args=(L1, L2, m1, m2, g))\n",
    "z=ret.y\n",
    "print(np.shape(z))\n",
    "\n",
    "# Extract result\n",
    "theta1, w1, theta2, w2 = z[0], z[1], z[2], z[3]\n",
    "x1, y1, x2, y2, vx1, vy1, vx2, vy2 = to_cartesian(theta1, w1, theta2, w2, L1, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(x1, y1, label=r\"Track $m_1$\")\n",
    "ax.plot(x2, y2, label=r\"Track $m_2$\")\n",
    "ax.plot([0, x1[0], x2[0]], [0, y1[0], y2[0]], \"-o\", label=\"Initial position\", c='k')\n",
    "plt.ylabel(r\"$y/L$\")\n",
    "plt.xlabel(r\"$x/L$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Predicting Chaos\n",
    "1. Design and train a recurrent neural network (of your choice) to predict the future positions, where future is defined as $t=t_0 + 20 \\delta t$, of the masses $m_1$ and $m_2$ using their cartesian coordinates and the initial conditions  $z_0=[\\pi/4,0,\\pi/4,0]$. \n",
    "2. How stable is your network to variations in initial conditions? Make a plot of $x$ and $y$ vs time to show the network prediction in comparison to the solution from solve_ivp\n",
    "3. How far into the future can a network predict? Make a plot showing how the deviation between predicted position and actual position (from solve_ivp above) vary as a function of extrapolation time from $t=t_0 + 20 \\delta t$ to $t=t_0 + 100 \\delta t$  (e.g. for each extrapolation time, train a new version of the network and then plot the performance)\n",
    "4. Repeat steps 1-3 for the initial conditions $z_0=[\\pi/2,0,\\pi/2,0]$ which give a much more complex path.\n",
    "5. Repeat steps 1-4 but only train your neural network on the cartesian coordinates of the mass $m_2$ (i.e without showing your neural network the positions of the mass $m_1$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Training an RNN for Predicting Future Positions  \n",
    "\n",
    "In this task, you will design and train a Recurrent Neural Network (RNN) to predict future positions based on initial mass coordinates.  \n",
    "\n",
    "The dataset consists of Cartesian coordinates extracted at various timesteps. Since RNNs process sequential data, we must structure the dataset into meaningful input-output pairs:  \n",
    "\n",
    "- **Input**: A sequence of positions over 20 consecutive timesteps  \n",
    "- **Output**: The predicted position at the next timestep  \n",
    "\n",
    "Your goal is to implement and train the RNN to accurately forecast future positions given the past trajectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stack extracted coordinates into a single dataset for training\n",
    "data = np.column_stack([x1, y1, x2, y2])  \n",
    "\n",
    "def prepare_data(data, offset=20, seq_length=20):\n",
    "    \"\"\"\n",
    "    Prepares input-output pairs for training an RNN using a sliding window approach.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The coordinate dataset.\n",
    "        offset (int): The gap between input sequence and target prediction.\n",
    "        seq_length (int): The length of each input sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Arrays (X, y) where:\n",
    "               - X contains sequences of past positions\n",
    "               - y contains the corresponding future positions\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # Iterate through the dataset while ensuring enough future data is available\n",
    "    for i in range(len(data) - seq_length - offset):\n",
    "        X.append(data[i : i + seq_length])  # Input: last `seq_length` positions\n",
    "        y.append(data[i + seq_length + offset])  # Output: position `offset` steps ahead\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare training data using a 20-step lookback and a 20-step prediction window\n",
    "X, y = prepare_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use LSTM for Time-Series Forecasting?  \n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are well-suited for time-series forecasting because they effectively capture **long-term dependencies** in sequential data. Unlike traditional RNNs, LSTMs use **gates** to regulate the flow of information, preventing issues like vanishing gradients and allowing the model to ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "def build_lstm_model():\n",
    "    \"\"\"\n",
    "    Builds and compiles an LSTM model for predicting future positions.\n",
    "    \n",
    "    Model Architecture:\n",
    "    - Input shape: (20 time steps, 4 coordinate features)\n",
    "    - Two LSTM layers for capturing temporal dependencies\n",
    "    - Fully connected (Dense) layers for refining predictions\n",
    "    - Final output layer with 4 neurons for coordinate prediction (x1, y1, x2, y2)\n",
    "    \n",
    "    Returns:\n",
    "        Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(20, 4)),  # Input layer with 20 timesteps and 4 features\n",
    "\n",
    "        # First LSTM layer: learns temporal patterns while preserving time dimension\n",
    "        LSTM(24, return_sequences=True),  \n",
    "\n",
    "        # Second LSTM layer: outputs only the final time step's result\n",
    "        LSTM(18, return_sequences=False),  \n",
    "\n",
    "        # Dense layer with ReLU activation to introduce non-linearity\n",
    "        Dense(12, activation='relu'),\n",
    "\n",
    "        # Output layer: 4 neurons to predict x1, y1, x2, y2\n",
    "        Dense(4)\n",
    "    ])\n",
    "\n",
    "    # Compile model with Adam optimizer and Mean Squared Error (MSE) loss\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "model = build_lstm_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Configuration  \n",
    "\n",
    "The model is trained with the following parameters:  \n",
    "\n",
    "- **Epochs**: 20 ‚Äì The number of times the model iterates over the entire dataset.  \n",
    "- **Batch Size**: 37 ‚Äì The number of samples processed before updating model weights.  \n",
    "\n",
    "These values are chosen to balance computational efficiency and model performance based on the dataset size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model using the fit function\n",
    "history = model.fit(\n",
    "    X, y,                 # Input (X) and target output (y)\n",
    "    epochs=30,            # Number of complete passes through the dataset\n",
    "    batch_size=10,        # Number of samples per batch for weight updates\n",
    "    validation_split=0.2, # Use 20% of the data for validation\n",
    "    verbose=1            # Display training progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and Evaluating Model Performance  \n",
    "\n",
    "### **Prediction**  \n",
    "The trained LSTM model is used to predict the values of **x1, y1, x2, and y2** for future timesteps.  \n",
    "\n",
    "### **Comparison**  \n",
    "To assess the model's accuracy, the predicted trajectory is compared against the ground truth obtained from **solve_ivp**, a numerical solver for differential equations. This comparison helps evaluate how well the model generalizes the dynamics of the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate predictions from the trained model\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Extract true positions from solve_ivp results (ground truth)\n",
    "true_x1, true_y1 = y[:, 0], y[:, 1]  # True positions of mass m1\n",
    "true_x2, true_y2 = y[:, 2], y[:, 3]  # True positions of mass m2\n",
    "\n",
    "# Extract predicted positions from the model\n",
    "pred_x1, pred_y1 = predictions[:, 0], predictions[:, 1]  # Predicted positions of mass m1\n",
    "pred_x2, pred_y2 = predictions[:, 2], predictions[:, 3]  # Predicted positions of mass m2\n",
    "\n",
    "# Plot x1 vs y1 for mass m1 (first mass)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(true_x1, true_y1, label=\"True Path (m1)\", linestyle=\"dashed\", color=\"blue\")\n",
    "plt.plot(pred_x1, pred_y1, label=\"Predicted Path (m1)\", color=\"red\", alpha=0.6)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"y1\")\n",
    "plt.title(\"Predicted vs. True Path of Mass m1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot x2 vs y2 for mass m2 (second mass)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(true_x2, true_y2, label=\"True Path (m2)\", linestyle=\"dashed\", color=\"blue\")\n",
    "plt.plot(pred_x2, pred_y2, label=\"Predicted Path (m2)\", color=\"red\", alpha=0.6)\n",
    "plt.xlabel(\"x2\")\n",
    "plt.ylabel(\"y2\")\n",
    "plt.title(\"Predicted vs. True Path of Mass m2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of the Predicted vs. True Path of the Double Pendulum**  \n",
    "\n",
    "## **Observations**  \n",
    "### **Trajectory Comparison**  \n",
    "- The **true paths** of the masses (*m‚ÇÅ* and *m‚ÇÇ*) are represented by dashed blue lines.  \n",
    "- The **predicted paths** from the LSTM model are shown in solid red.  \n",
    "\n",
    "### **Mass m‚ÇÅ (Top Plot)**  \n",
    "- The predicted trajectory follows the general shape of the true path but exhibits **significant divergence and noise**.  \n",
    "- Predictions appear scattered, especially in the middle region, indicating **instability in long-term motion prediction**.  \n",
    "- The fluctuations in red lines suggest **error accumulation over time**, which may be due to limitations in the LSTM's ability to capture long-term dependencies.  \n",
    "\n",
    "### **Mass m‚ÇÇ (Bottom Plot)**  \n",
    "- The predicted paths for *m‚ÇÇ* align more closely with the true trajectory compared to *m‚ÇÅ*.  \n",
    "- While some **variation and slight divergence** are present, the overall structure of the motion is preserved better.  \n",
    "- This could indicate that *m‚ÇÇ* follows a more predictable pattern, making it easier for the model to learn.  \n",
    "\n",
    "## **Potential Issues and Recommendations**  \n",
    "\n",
    "###  **Prediction Deviation**  \n",
    "- The model struggles to maintain accuracy, especially for *m‚ÇÅ*.  \n",
    "- **Possible Causes**:\n",
    "  - The LSTM might not have effectively learned **long-term dependencies**.  \n",
    "  - Training data may need more **diverse trajectories** to improve generalization.  \n",
    "  - The model may benefit from using **longer input sequences** (more than 20 timesteps).  \n",
    "\n",
    "###  **Noise and Instability in Predictions**  \n",
    "- The fluctuations suggest that the model may be **overfitting to short-term patterns** rather than learning smooth motion.  \n",
    "- **Possible Fixes**:\n",
    "  - Apply **regularization techniques** (dropout, L2 regularization) to improve generalization.  \n",
    "  - Use **a more complex architecture** (e.g., bidirectional LSTM or Transformer-based approaches).  \n",
    "  - Incorporate **physical constraints** (such as energy conservation) into model training.  \n",
    "\n",
    "### **Better Evaluation Metrics**  \n",
    "- Instead of relying solely on visual comparison, additional evaluation metrics should be considered:  \n",
    "  - **Mean Squared Error (MSE)** between predicted and true values.  \n",
    "  - **Trajectory similarity measures** (such as Dynamic Time Warping).  \n",
    "\n",
    "## **Conclusion**  \n",
    "- The model captures the **overall structure** of the double pendulum‚Äôs motion but struggles with **long-term accuracy and stability**.  \n",
    "- Improvements in **data preprocessing, model architecture, and hyperparameter tuning** could enhance its predictive performance. üöÄ  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2: Evaluating Network Stability to Variations in Initial Conditions**  \n",
    "\n",
    "To assess the robustness of the trained network, we introduce variations in the **initial conditions** and analyze how well the model generalizes.  \n",
    "\n",
    "### **Objective**  \n",
    "- Determine how sensitive the LSTM model is to slight changes in initial positions and velocities.  \n",
    "- Compare the model‚Äôs predicted trajectories with numerically computed solutions.  \n",
    "\n",
    "### **Approach**  \n",
    "- Implement a function that utilizes **solve_ivp** to compute the Cartesian coordinates of the system.  \n",
    "- Compare results across different initial conditions to assess model stability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_pendulum(z0):\n",
    "    \"\"\"\n",
    "    Solves the double pendulum system using solve_ivp.\n",
    "\n",
    "    Parameters:\n",
    "        z0 (numpy.ndarray): Initial conditions [Œ∏1, œâ1, Œ∏2, œâ2].\n",
    "\n",
    "    Returns:\n",
    "        tuple: Cartesian coordinates (x1, y1, x2, y2) and velocities (vx1, vy1, vx2, vy2).\n",
    "    \"\"\"\n",
    "    ret = solve_ivp(rhs, (0, tmax), z0, t_eval=t, args=(L1, L2, m1, m2, g))\n",
    "    z = ret.y\n",
    "    return to_cartesian(z[0], z[1], z[2], z[3], L1, L2)\n",
    "\n",
    "# Define perturbation levels in terms of œÄ (since angles are in radians)\n",
    "perturbation_levels = [2 * np.pi * factor for factor in [0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 0.9, 1]]\n",
    "\n",
    "def test_perturbations(model, z0, perturbation_levels, solve_pendulum, prepare_data):\n",
    "    \"\"\"\n",
    "    Evaluates model stability under different perturbations in initial conditions.\n",
    "\n",
    "    Parameters:\n",
    "        model (Keras Model): Trained LSTM model.\n",
    "        z0 (numpy.ndarray): Initial conditions [Œ∏1, œâ1, Œ∏2, œâ2].\n",
    "        perturbation_levels (list): List of perturbation magnitudes.\n",
    "        solve_pendulum (function): Function to compute true trajectories.\n",
    "        prepare_data (function): Function to format data for LSTM input.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))  # 2 rows, 4 columns of subplots\n",
    "\n",
    "    for idx, delta in enumerate(perturbation_levels):\n",
    "        row, col = divmod(idx, 4)  # Convert index to row and column position\n",
    "\n",
    "        # Apply perturbation to the initial conditions\n",
    "        z0_perturbed = np.array([z0[0] + delta, z0[1], z0[2] + delta, z0[3]])\n",
    "\n",
    "        # Solve for the true trajectory\n",
    "        x1, y1, x2, y2, vx1, vy1, vx2, vy2 = solve_pendulum(z0_perturbed)\n",
    "\n",
    "        # Prepare data for the LSTM model\n",
    "        data_perturbed = np.column_stack([x1, y1, x2, y2])\n",
    "        X_perturbed, y_perturbed = prepare_data(data_perturbed, offset=20, seq_length=20)\n",
    "        X_perturbed = np.array(X_perturbed)\n",
    "\n",
    "        # Predict future positions using the trained model\n",
    "        predicted_pos = model.predict(X_perturbed, verbose=0)\n",
    "\n",
    "        # Plot the true vs predicted trajectories\n",
    "        axes[row, col].plot(x1, y1, linestyle=\"dashed\", label=\"True m1\", color=\"blue\")\n",
    "        axes[row, col].plot(predicted_pos[:, 0], predicted_pos[:, 1], label=\"Predicted m1\", color=\"red\")\n",
    "        axes[row, col].plot(x2, y2, linestyle=\"dashed\", label=\"True m2\", color=\"green\")\n",
    "        axes[row, col].plot(predicted_pos[:, 2], predicted_pos[:, 3], label=\"Predicted m2\", color=\"orange\")\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        axes[row, col].set_xlabel(\"x\")\n",
    "        axes[row, col].set_ylabel(\"y\")\n",
    "        axes[row, col].set_title(f\"Perturbation: {delta:.3f}\")\n",
    "        axes[row, col].legend()\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "# Run the perturbation analysis\n",
    "test_perturbations(model, z0, perturbation_levels, solve_pendulum, prepare_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Model Stability Under Perturbations**  \n",
    "\n",
    "## **Observations**  \n",
    "\n",
    "### **Effect of Small Perturbations (ùõø ‚âà 0.063 - 0.628)**  \n",
    "- The predicted paths closely follow the true trajectories with minor deviations.  \n",
    "- The model captures short-term dynamics well but begins to show slight divergence over time.  \n",
    "\n",
    "### **Moderate Perturbations (ùõø ‚âà 1.257 - 3.142)**  \n",
    "- The predicted paths start to deviate more significantly from the true trajectories.  \n",
    "- The model struggles to maintain accuracy, with increasing instability in predictions.  \n",
    "- The motion remains somewhat structured but does not accurately match the expected results.  \n",
    "\n",
    "### **Large Perturbations (ùõø ‚âà 4.712 - 6.283)**  \n",
    "- The predicted paths become highly erratic and fail to resemble the true trajectories.  \n",
    "- The model does not generalize well beyond a certain threshold of initial condition variation.  \n",
    "- The system's true motion remains structured, but the predicted paths appear inconsistent.  \n",
    "\n",
    "## **Potential Issues and Recommendations**  \n",
    "\n",
    "### **Error Accumulation in Long-Term Predictions**  \n",
    "- Small initial errors increase over time, leading to significant deviations.  \n",
    "- A possible improvement is to use sequence-to-sequence models that correct errors at each step.  \n",
    "\n",
    "### **Overfitting to Specific Initial Conditions**  \n",
    "- The model performs well for trained conditions but struggles with unseen perturbations.  \n",
    "- Training on a wider range of initial conditions and using data augmentation may improve generalization.  \n",
    "\n",
    "### **Sensitivity to Angular Perturbations**  \n",
    "- Predictions degrade faster with larger changes in initial angles.  \n",
    "- Introducing physics-based constraints or additional training data covering a wider range of angles could help.  \n",
    "\n",
    "## **Conclusion**  \n",
    "- The model is stable for small perturbations but fails for larger deviations.  \n",
    "- Improvements in training data diversity and model architecture may enhance robustness.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3: Evaluating Prediction Horizon**  \n",
    "\n",
    "## **Objective**  \n",
    "To determine how far into the future the trained network can make reliable predictions before errors accumulate.  \n",
    "\n",
    "## **Approach**  \n",
    "- Define different extrapolation times to assess how well the model generalizes over extended time horizons.  \n",
    "- Use a loop to iterate through each extrapolation time and compare the predicted positions with the true trajectories.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extrapolation steps from t0 + 20dt to t0 + 100dt\n",
    "extrapolation_steps = [20, 30, 40, 50, 60, 70, 80, 90, 100] \n",
    "\n",
    "def extrapolation(extrapolation_steps, data, lstm_model, prepare_data):\n",
    "    \"\"\"\n",
    "    Evaluates how far into the future the LSTM model can make reliable predictions.\n",
    "\n",
    "    Parameters:\n",
    "        extrapolation_steps (list): List of time steps for prediction.\n",
    "        data (numpy.ndarray): Input data containing positions of masses.\n",
    "        lstm_model (function): Function to create an LSTM model.\n",
    "        prepare_data (function): Function to structure data for training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lists containing errors for m1 and m2, overall MSE values, and training loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Variables to store errors and training loss\n",
    "    errors_m1, errors_m2, mse_values, cost = [], [], [], []\n",
    "\n",
    "    # Create subplots for trajectory visualization\n",
    "    fig, axes = plt.subplots(len(extrapolation_steps), 2, figsize=(10, len(extrapolation_steps) * 4))\n",
    "\n",
    "    for idx, steps in enumerate(extrapolation_steps):\n",
    "        print(f\"Training LSTM model for extrapolation step {steps}...\")\n",
    "\n",
    "        # Prepare input-output data for the given extrapolation step\n",
    "        X_train, Y_train = prepare_data(data, steps)\n",
    "\n",
    "        # Initialize a new model for each extrapolation step\n",
    "        model = lstm_model()\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(X_train, Y_train, epochs=15, batch_size=10, verbose=0)\n",
    "        loss = history.history['loss'][-1]  # Extract final training loss\n",
    "\n",
    "        # Predict future positions\n",
    "        Y_pred = model.predict(X_train)\n",
    "\n",
    "        # Compute Mean Squared Error (MSE) for both masses\n",
    "        mse_m1 = np.mean((Y_train[:, :2] - Y_pred[:, :2]) ** 2)  # Error for m1\n",
    "        mse_m2 = np.mean((Y_train[:, 2:] - Y_pred[:, 2:]) ** 2)  # Error for m2\n",
    "        mse = np.mean((Y_train - Y_pred) ** 2)  # Overall error\n",
    "\n",
    "        # Store errors and training loss\n",
    "        errors_m1.append(mse_m1)\n",
    "        errors_m2.append(mse_m2)\n",
    "        mse_values.append(mse)\n",
    "        cost.append(loss)\n",
    "\n",
    "        # Plot actual vs predicted trajectory for m1\n",
    "        ax1 = axes[idx, 0]\n",
    "        ax1.plot(Y_train[:, 0], Y_train[:, 1], linestyle=\"dashed\", label=\"True m1\", color=\"blue\")\n",
    "        ax1.plot(Y_pred[:, 0], Y_pred[:, 1], linestyle=\"solid\", label=\"Predicted m1\", color=\"red\")\n",
    "        ax1.set_xlabel(\"x1\")\n",
    "        ax1.set_ylabel(\"y1\")\n",
    "        ax1.set_title(f\"m1 Trajectory (t0 + {steps}dt)\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot actual vs predicted trajectory for m2\n",
    "        ax2 = axes[idx, 1]\n",
    "        ax2.plot(Y_train[:, 2], Y_train[:, 3], linestyle=\"dashed\", label=\"True m2\", color=\"green\")\n",
    "        ax2.plot(Y_pred[:, 2], Y_pred[:, 3], linestyle=\"solid\", label=\"Predicted m2\", color=\"orange\")\n",
    "        ax2.set_xlabel(\"x2\")\n",
    "        ax2.set_ylabel(\"y2\")\n",
    "        ax2.set_title(f\"m2 Trajectory (t0 + {steps}dt)\")\n",
    "        ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return errors_m1, errors_m2, mse_values, cost\n",
    "\n",
    "# Run extrapolation analysis\n",
    "errors_m1, errors_m2, mse_values, cost = extrapolation(extrapolation_steps, data, lstm_model, prepare_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Squared Error (MSE) vs. Extrapolation Time\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot errors for m1, m2, and combined error\n",
    "plt.plot(extrapolation_steps, errors_m1, marker='o', linestyle='--', label='Error for m1', color='blue')\n",
    "plt.plot(extrapolation_steps, errors_m2, marker='o', linestyle='--', label='Error for m2', color='orange')\n",
    "plt.plot(extrapolation_steps, mse_values, marker='o', linestyle='-', label='Combined error', color='green')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Extrapolation Steps (t0 + _dt)\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Prediction Error vs. Extrapolation Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Prediction Error vs. Extrapolation Time**  \n",
    "\n",
    "## **Observations**  \n",
    "\n",
    "### **Trend of Prediction Error**  \n",
    "- The **mean squared error (MSE)** is plotted against different extrapolation steps (t0 + dt).  \n",
    "- Errors for both **m1 (blue)** and **m2 (orange)**, as well as the **combined error (green)**, are shown.  \n",
    "- The error fluctuates with increasing extrapolation time, rather than growing consistently.  \n",
    "\n",
    "### **Short-Term Predictions (t0 + 20dt to t0 + 60dt)**  \n",
    "- MSE remains relatively low and stable within this range.  \n",
    "- There is a slight dip around **t0 + 60dt**, indicating the model performs better at this point.  \n",
    "- Error fluctuations suggest the model may have learned periodic patterns within this timeframe.  \n",
    "\n",
    "### **Longer-Term Predictions (t0 + 70dt to t0 + 100dt)**  \n",
    "- A **sharp increase in error** is observed beyond **t0 + 70dt**, peaking near **t0 + 90dt to t0 + 100dt**.  \n",
    "- This indicates that the model struggles to maintain accurate long-term predictions.  \n",
    "- The rising error suggests an accumulation of small deviations, leading to greater inaccuracy.  \n",
    "\n",
    "## **Potential Issues and Recommendations**  \n",
    "\n",
    "### **Error Accumulation Over Time**  \n",
    "- The increasing error at longer extrapolation times suggests that small prediction inaccuracies compound over multiple steps.  \n",
    "- A possible solution is **sequence-to-sequence models** that correct errors dynamically instead of relying solely on past predictions.  \n",
    "\n",
    "### **Fluctuations in Error**  \n",
    "- The dips in error at certain time steps indicate that the model may have learned specific patterns better than others.  \n",
    "- Training on **a wider range of initial conditions and time steps** could help smooth out these fluctuations.  \n",
    "\n",
    "### **Loss of Predictive Stability Beyond t0 + 70dt**  \n",
    "- The model appears to have a limit on how far it can generalize before errors become dominant.  \n",
    "- **Incorporating physical constraints** into the training process might improve long-term stability.  \n",
    "\n",
    "## **Conclusion**  \n",
    "- The model performs reasonably well for **short-term predictions**, but its reliability decreases for **longer forecasting horizons**.  \n",
    "- Refining the model architecture and training process could improve stability and reduce long-term prediction errors.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting Loss vs Extrapolation Steps\n",
    "plt.figure(figsize=(8, 5))  # Set figure size\n",
    "plt.plot(extrapolation_steps, cost, marker='o', linestyle='--')\n",
    "plt.xlabel(\"Extrapolation Steps (t0 + _dt)\" )\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Prediction Loss vs Extrapolation Time\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Prediction Loss vs. Extrapolation Time**  \n",
    "\n",
    "## **Observations**  \n",
    "\n",
    "### **Trend of Prediction Loss**  \n",
    "- The loss fluctuates across different extrapolation steps, indicating that the model performs inconsistently at different time horizons.  \n",
    "- The loss decreases at **t0 + 30dt and t0 + 60dt**, suggesting that the model maintains reasonable accuracy at these steps.  \n",
    "- A significant **increase in loss beyond t0 + 70dt** suggests that the model struggles with long-term predictions.  \n",
    "- The peak at **t0 + 90dt** shows the highest error, indicating reduced stability for further extrapolations.  \n",
    "\n",
    "## **Potential Issues and Recommendations**  \n",
    "\n",
    "### **Instability in Long-Term Predictions**  \n",
    "- The increasing loss at higher extrapolation times suggests that the model‚Äôs predictions become unreliable for longer forecasting horizons.  \n",
    "- A possible solution is to use **attention-based models** or **physics-informed neural networks** to improve long-term stability.  \n",
    "\n",
    "### **Fluctuations in Loss**  \n",
    "- The drops in loss at certain extrapolation steps suggest the model has learned periodic patterns but does not generalize well across all time steps.  \n",
    "- Training on a **wider range of initial conditions** may help smooth out these fluctuations.  \n",
    "\n",
    "### **Deterioration After t0 + 70dt**  \n",
    "- The loss rises significantly after **t0 + 70dt**, indicating that the model is less effective at capturing long-term dependencies.  \n",
    "- Using **longer input sequences** or **stateful LSTMs** might help retain important temporal information.  \n",
    "\n",
    "## **Conclusion**  \n",
    "- The model performs well for **short-term predictions**, but its accuracy decreases with longer forecasting horizons.  \n",
    "- Refinements in model architecture, training strategy, and data augmentation could improve long-term prediction performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 4: Evaluating Model Performance on a Different Set of Initial Conditions**  \n",
    "\n",
    "## **Objective**  \n",
    "To assess the generalizability of the trained LSTM model by repeating the previous analyses (**Exercises 1-3**) with a new set of initial conditions.  \n",
    "\n",
    "## **Approach**  \n",
    "1. **Train and Test the Model on New Initial Conditions**  \n",
    "   - Extract and structure new coordinate data for training.  \n",
    "   - Train an LSTM model to predict future positions.  \n",
    "\n",
    "2. **Evaluate Model Stability to Variations in Initial Conditions**  \n",
    "   - Introduce small perturbations in the new initial conditions.  \n",
    "   - Compare predicted trajectories with numerically computed solutions.  \n",
    "\n",
    "3. **Analyze the Model‚Äôs Extrapolation Capability**  \n",
    "   - Test how far into the future the model can predict before errors accumulate.  \n",
    "   - Compute and visualize error trends over different extrapolation steps.  \n",
    "\n",
    "## **Expected Insights**  \n",
    "- Determine whether the model maintains similar accuracy across different initial conditions.  \n",
    "- Identify if errors and stability issues follow the same patterns as in previous experiments.  \n",
    "- Evaluate whether additional training on varied initial conditions improves generalization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update initial conditions for Task 4\n",
    "z0_task4 = [np.pi / 2, 0, np.pi / 2, 0]\n",
    "\n",
    "# Solve the double pendulum system for new initial conditions\n",
    "x1_4, y1_4, x2_4, y2_4, vx1, vy1, vx2, vy2 = solve_pendulum(z0_task4)\n",
    "\n",
    "# Prepare LSTM training data\n",
    "data_task4 = np.column_stack([x1_4, y1_4, x2_4, y2_4])\n",
    "X_task4, y_task4 = prepare_data(data_task4, offset=20, seq_length=20)\n",
    "\n",
    "# Create and train a new LSTM model\n",
    "model_task4 = lstm_model()\n",
    "model_task4.fit(X_task4, y_task4, epochs=30, batch_size=10, verbose=0)\n",
    "\n",
    "# Predict future positions using the trained model\n",
    "Y_pred_4 = model_task4.predict(X_task4)\n",
    "\n",
    "# Plot x1 vs y1 for mass m1\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x1_4, y1_4, label=\"True path (m1)\", linestyle=\"dashed\", color=\"blue\")\n",
    "plt.plot(Y_pred_4[:, 0], Y_pred_4[:, 1], label=\"Predicted path (m1)\", color=\"red\", alpha=0.6)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"y1\")\n",
    "plt.legend()\n",
    "plt.title(\"Predicted vs. True Path of Mass m1\")\n",
    "plt.show()\n",
    "\n",
    "# Plot x2 vs y2 for mass m2\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x2_4, y2_4, label=\"True path (m2)\", linestyle=\"dashed\", color=\"green\")\n",
    "plt.plot(Y_pred_4[:, 2], Y_pred_4[:, 3], label=\"Predicted path (m2)\", color=\"red\", alpha=0.6)\n",
    "plt.xlabel(\"x2\")\n",
    "plt.ylabel(\"y2\")\n",
    "plt.legend()\n",
    "plt.title(\"Predicted vs. True Path of Mass m2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perturbations(model_task4, z0_task4, pertubation, solve_pendulum, prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_m1_4, errors_m2_4, mse_values_4, cost_4 = extrapolation(extrapolation_steps, data_task4, lstm_model, prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting error vs extrapolation time\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(extrapolation_steps, errors_m1_4, marker='o', linestyle='--', label='Error for m1')\n",
    "plt.plot(extrapolation_steps, errors_m2_4, marker='o', linestyle='--', label='Error for m2')\n",
    "plt.plot(extrapolation_steps, mse_values_4, marker='o', label='Combined error')\n",
    "plt.xlabel(\"Extrapolation Steps (t0 + _dt)\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Prediction Error vs Extrapolation Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Loss vs Extrapolation Steps\n",
    "plt.figure(figsize=(8, 5))  # Set figure size\n",
    "plt.plot(extrapolation_steps, cost_4, marker='o', linestyle='--')\n",
    "plt.xlabel(\"Extrapolation Steps (t0 + _dt)\" )\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Prediction Loss vs Extrapolation Time\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Prediction Error and Loss vs. Extrapolation Time (New Initial Conditions)**  \n",
    "\n",
    "## **Observations**  \n",
    "\n",
    "### **Prediction Error (Top Plot)**  \n",
    "- The **mean squared error (MSE)** increases as extrapolation time increases.  \n",
    "- The error for **m1 (blue)** remains relatively low compared to **m2 (orange)**, which exhibits a steeper increase.  \n",
    "- The **combined error (green)** follows an upward trend, indicating a general loss of accuracy over time.  \n",
    "- Unlike the previous analysis, error accumulation is more pronounced for **m2**, suggesting that the new initial conditions result in greater instability in the second mass.  \n",
    "\n",
    "### **Prediction Loss (Bottom Plot)**  \n",
    "- The overall loss increases steadily as extrapolation time increases, reaching its peak near **t0 + 80dt to 90dt**.  \n",
    "- There is a slight decline at **t0 + 100dt**, but the loss remains significantly higher than at shorter time steps.  \n",
    "- This suggests that the model struggles to maintain stability for **long-term predictions** under the new initial conditions.  \n",
    "\n",
    "## **Potential Issues and Recommendations**  \n",
    "\n",
    "### **Greater Instability for m2**  \n",
    "- The higher error in predicting **m2** suggests that the second mass is more sensitive to perturbations in this setup.  \n",
    "- Training the model with a **wider range of initial conditions** could help improve robustness.  \n",
    "\n",
    "### **Steady Increase in Loss**  \n",
    "- The continuous rise in loss indicates that **long-term dependencies are not well captured** by the LSTM model.  \n",
    "- Using **attention mechanisms or recurrent dropout** might help prevent performance degradation over time.  \n",
    "\n",
    "### **Effect of Different Initial Conditions**  \n",
    "- Compared to previous results, the model appears **less stable under these new conditions**.  \n",
    "- Testing across multiple initial conditions may provide insight into whether **certain configurations lead to more predictable behavior**.  \n",
    "\n",
    "## **Conclusion**  \n",
    "- The model performs well for **short-term predictions**, but its accuracy declines as the extrapolation time increases.  \n",
    "- Future improvements could involve **data augmentation, physics-informed modeling, or alternative architectures** to handle instability better.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 5: Training the Model Using Only m2 Data**  \n",
    "\n",
    "## **Objective**  \n",
    "To evaluate how well the LSTM model can predict the trajectory of **m2** when trained exclusively on its data. This will help assess whether focusing on a single mass improves predictive accuracy or if the model still struggles with long-term stability.  \n",
    "\n",
    "## **Approach**  \n",
    "1. **Train the Model on m2 Data**  \n",
    "   - Extract and structure **only x2, y2** from the dataset.  \n",
    "   - Train an LSTM model using only the second mass‚Äôs trajectory.  \n",
    "\n",
    "2. **Evaluate Model Stability Under Perturbations**  \n",
    "   - Introduce variations in initial conditions and compare predicted vs. true trajectories.  \n",
    "   - Analyze whether focusing on m2 improves stability in predictions.  \n",
    "\n",
    "3. **Test the Model‚Äôs Extrapolation Ability**  \n",
    "   - Assess how far into the future the model can predict before errors accumulate.  \n",
    "   - Compare the **prediction error** and **training loss** against previous experiments.  \n",
    "\n",
    "4. **Evaluate Generalization Across Initial Conditions**  \n",
    "   - Repeat the analysis using a different set of initial conditions.  \n",
    "   - Compare whether training only on m2 leads to more reliable long-term forecasts.  \n",
    "\n",
    "## **Expected Insights**  \n",
    "- Determine if isolating m2‚Äôs data **reduces predictive errors** compared to training on both masses.  \n",
    "- Evaluate whether the model becomes more stable when trained on a **single trajectory instead of both**.  \n",
    "- Identify whether m2‚Äôs trajectory is inherently more predictable or if similar instability issues arise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model_5():\n",
    "    \"\"\"\n",
    "    Creates and compiles an LSTM model to predict m2's trajectory (x2, y2).\n",
    "\n",
    "    Returns:\n",
    "        Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(20, 2)),  # Input shape (20 timesteps, 2 features)\n",
    "        LSTM(24, return_sequences=True),  # First LSTM layer, extracts temporal patterns\n",
    "        LSTM(18, return_sequences=False),  # Second LSTM layer, outputs final state\n",
    "        Dense(12, activation='relu'),  # Dense layer for non-linearity\n",
    "        Dense(2)  # Output layer (predicting x2, y2)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')  # Adam optimizer with MSE loss\n",
    "    return model\n",
    "\n",
    "# Define initial conditions for task 5\n",
    "z0_task5_1 = [np.pi/4, 0, np.pi/4, 0]\n",
    "z0_task5_2 = [np.pi/2, 0, np.pi/2, 0] \n",
    "\n",
    "# Solve the double pendulum system for the given initial conditions\n",
    "x1_1, y1_1, x2_1, y2_1, vx1, vy1, vx2, vy2 = solve_pendulum(z0_task5_1)\n",
    "x1_2, y1_2, x2_2, y2_2, vx1, vy1, vx2, vy2 = solve_pendulum(z0_task5_2)\n",
    "\n",
    "# Prepare LSTM training data using only m2's coordinates (x2, y2)\n",
    "data_task5_1 = np.column_stack([x2_1, y2_1])  \n",
    "data_task5_2 = np.column_stack([x2_2, y2_2])  \n",
    "X_task5_1, y_task5_1 = prepare_data(data_task5_1, offset=20, seq_length=20)\n",
    "X_task5_2, y_task5_2 = prepare_data(data_task5_2, offset=20, seq_length=20)\n",
    "\n",
    "# Create and train models separately for both initial conditions\n",
    "model_task5_1 = lstm_model_5()\n",
    "model_task5_2 = lstm_model_5()\n",
    "model_task5_1.fit(X_task5_1, y_task5_1, epochs=30, batch_size=10, verbose=0)\n",
    "model_task5_2.fit(X_task5_2, y_task5_2, epochs=30, batch_size=10, verbose=0)\n",
    "\n",
    "# Predict the future values for m2\n",
    "Y_pred_5_1 = model_task5_1.predict(X_task5_1)\n",
    "Y_pred_5_2 = model_task5_2.predict(X_task5_2)\n",
    "\n",
    "# Create subplots for visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot for first initial condition (z0_task5_1)\n",
    "axes[0].plot(x1_1, y1_1, linestyle=\"dashed\", color=\"blue\", label=\"True m1 (reference)\")\n",
    "axes[0].plot(data_task5_1[:, 0], data_task5_1[:, 1], linestyle=\"dashed\", color=\"green\", label=\"True m2\")\n",
    "axes[0].plot(Y_pred_5_1[:, 0], Y_pred_5_1[:, 1], linestyle=\"solid\", color=\"red\", label=\"Predicted m2\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "axes[0].set_title(\"Predicted vs. True (m2) - Initial Condition 1\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot for second initial condition (z0_task5_2)\n",
    "axes[1].plot(x1_2, y1_2, linestyle=\"dashed\", color=\"blue\", label=\"True m1 (reference)\")\n",
    "axes[1].plot(data_task5_2[:, 0], data_task5_2[:, 1], linestyle=\"dashed\", color=\"green\", label=\"True m2\")\n",
    "axes[1].plot(Y_pred_5_2[:, 0], Y_pred_5_2[:, 1], linestyle=\"solid\", color=\"red\", label=\"Predicted m2\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"y\")\n",
    "axes[1].set_title(\"Predicted vs. True (m2) - Initial Condition 2\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout and show plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with pertubations for initial condition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perturbations(perturbation_levels, z0_task, model, solve_pendulum, prepare_data):\n",
    "    \"\"\"\n",
    "    Evaluates the effect of perturbations on LSTM predictions and visualizes actual vs. predicted trajectories.\n",
    "\n",
    "    Parameters:\n",
    "        perturbation_levels (list): List of perturbation magnitudes.\n",
    "        z0_task (numpy.ndarray): Initial conditions of the system.\n",
    "        model (Keras Model): Trained LSTM model for m2.\n",
    "        solve_pendulum (function): Function to compute true trajectories.\n",
    "        prepare_data (function): Function to structure data for LSTM input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create subplots (2 rows, 4 columns) for visualizing different perturbations\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "\n",
    "    for idx, delta in enumerate(perturbation_levels):\n",
    "        row, col = divmod(idx, 4)  # Map index to subplot grid position\n",
    "\n",
    "        # Apply perturbation to the initial conditions\n",
    "        z0_perturbation = np.array([z0_task[0] + delta, z0_task[1], z0_task[2] + delta, z0_task[3]])\n",
    "\n",
    "        # Solve for the true perturbed trajectory\n",
    "        x1, y1, x2, y2, vx1, vy1, vx2, vy2 = solve_pendulum(z0_perturbation)\n",
    "\n",
    "        # Prepare data for the LSTM model using only m2's coordinates\n",
    "        data_perturbation = np.column_stack([x2, y2])\n",
    "        X_perturbation, _ = prepare_data(data_perturbation, offset=20, seq_length=20)\n",
    "        X_perturbation = np.array(X_perturbation)\n",
    "\n",
    "        # Predict m2's future positions using the trained model\n",
    "        predicted_pos = model.predict(X_perturbation, verbose=0)\n",
    "\n",
    "        # Plot actual vs predicted trajectory for m2\n",
    "        axes[row, col].plot(x2, y2, linestyle=\"dashed\", label=\"True m2\", color=\"green\")\n",
    "        axes[row, col].plot(predicted_pos[:, 0], predicted_pos[:, 1], linestyle=\"solid\", label=\"Predicted m2\", color=\"red\")\n",
    "        \n",
    "        # Add labels and title\n",
    "        axes[row, col].set_xlabel(\"x\")\n",
    "        axes[row, col].set_ylabel(\"y\")\n",
    "        axes[row, col].set_title(f\"Perturbation: {delta:.3f}\")\n",
    "        axes[row, col].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate perturbations for the trained model on initial condition 1\n",
    "evaluate_perturbations(pertubation, z0_task5_1, model_task5_1, solve_pendulum, prepare_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with pertubations for initial condition 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_perturbations(pertubation, z0_task5_2, model_task5_2, solve_pendulum, prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extrapolation(extrapolation_steps, data_task, lstm_model_5, prepare_data):\n",
    "    \"\"\"Evaluates the effect of extrapolation steps on the prediction accuracy of the LSTM model\"\"\"\n",
    "    \n",
    "    #creating variables to store errors for plotting\n",
    "    mse_values = [] \n",
    "    cost = []\n",
    "\n",
    "    #creating a figure with subplots for trajectories\n",
    "    fig, axes = plt.subplots(len(extrapolation_steps), 1, figsize=(10, len(extrapolation_steps) * 4))\n",
    "\n",
    "    #using a for loop to go over all the different extrapolation times\n",
    "    for idx, steps in enumerate(extrapolation_steps):\n",
    "        #to indicate that the code is working \n",
    "        print(f\"Training LSTM model for extrapolation step {steps}...\")\n",
    "        #using the function to obtaining training datasets\n",
    "        X_train, Y_train = prepare_data(data_task, steps)\n",
    "\n",
    "        #creating a new model for each extrapolation time\n",
    "        model = lstm_model_5()\n",
    "\n",
    "        #training the model on the data extracted\n",
    "        history = model.fit(X_train, Y_train, epochs=15, batch_size=10, verbose=0)\n",
    "        loss = history.history['loss'][-1]\n",
    "\n",
    "        #predicting the future values\n",
    "        Y_pred = model.predict(X_train)\n",
    "\n",
    "        #finding the mean squared error (MSE)\n",
    "        mse = np.mean((Y_train - Y_pred) ** 2)\n",
    "        mse_values.append(mse)\n",
    "        cost.append(loss)\n",
    "\n",
    "        #plotting actual vs predicted trajectory for m2\n",
    "        ax = axes[idx]\n",
    "        ax.plot(Y_train[:, 0], Y_train[:, 1], linestyle=\"dashed\", label=\"True m2\", color=\"blue\")\n",
    "        ax.plot(Y_pred[:, 0], Y_pred[:, 1], linestyle=\"solid\", label=\"Predicted m2\", color=\"red\")\n",
    "        ax.set_xlabel(\"x1\")\n",
    "        ax.set_ylabel(\"y1\")\n",
    "        ax.set_title(f\"m2 Trajectory (t0 + {steps}dt)\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mse_values, cost\n",
    "\n",
    "mse_values_51, cost51 = evaluate_extrapolation(extrapolation_steps, data_task5_1, lstm_model_5, prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values_52, cost52 = evaluate_extrapolation(extrapolation_steps, data_task5_2, lstm_model_5, prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mean Squared Error (MSE) vs. Extrapolation Steps for both initial conditions on the same plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(extrapolation_steps, mse_values_51, marker='o', linestyle='-', color='blue', label=\"IC 1: z0 = [œÄ/4, 0, œÄ/4, 0]\")\n",
    "plt.plot(extrapolation_steps, mse_values_52, marker='o', linestyle='-', color='red', label=\"IC 2: z0 = [œÄ/2, 0, œÄ/2, 0]\")\n",
    "\n",
    "plt.xlabel(\"Extrapolation Steps (t0 + _dt)\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Prediction Error vs. Extrapolation Time\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss vs Extrapolation Steps for both initial conditions on the same plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(extrapolation_steps, cost51, marker='o', linestyle='--', color='blue', label='IC 1: z0 = [œÄ/4, 0, œÄ/4, 0]')\n",
    "plt.plot(extrapolation_steps, cost52, marker='o', linestyle='--', color='red', label='IC 2: z0 = [œÄ/2, 0, œÄ/2, 0]')\n",
    "\n",
    "plt.xlabel(\"Extrapolation Steps (t0 + _dt)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Prediction Loss vs. Extrapolation Time\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Prediction Error and Loss vs. Extrapolation Time**  \n",
    "\n",
    "## **Observations**  \n",
    "\n",
    "### **Prediction Error (Top Plot)**\n",
    "- **Initial Condition 1 (Blue Line)**  \n",
    "  - The **Mean Squared Error (MSE)** remains consistently low across all extrapolation steps.  \n",
    "  - There are minor fluctuations, but the error does not increase significantly over time.  \n",
    "  - This suggests that the model predicts well for this initial condition and does not suffer from severe long-term instability.  \n",
    "\n",
    "- **Initial Condition 2 (Red Line)**  \n",
    "  - The MSE is significantly higher than in Initial Condition 1.  \n",
    "  - The error fluctuates slightly but generally follows an **increasing trend**, indicating growing prediction instability over time.  \n",
    "  - At **t0 + 100dt**, the error reaches its peak, showing that long-term predictions become highly unreliable for this condition.  \n",
    "\n",
    "### **Prediction Loss (Bottom Plot)**\n",
    "- **Initial Condition 1 (Blue Line)**  \n",
    "  - The loss remains very low throughout all extrapolation steps.  \n",
    "  - The small fluctuations suggest **slight variations in learning performance**, but the model remains stable.  \n",
    "\n",
    "- **Initial Condition 2 (Red Line)**  \n",
    "  - The loss is **consistently higher**, following a pattern similar to the error plot.  \n",
    "  - The loss increases steadily with extrapolation time, confirming that the model struggles with **long-term accuracy** in this case.  \n",
    "\n",
    "## **Key Insights**  \n",
    "\n",
    "1. **Model Stability Varies with Initial Conditions**  \n",
    "   - The model performs **significantly better** for **Initial Condition 1** compared to **Initial Condition 2**.  \n",
    "   - This suggests that certain initial conditions **lead to more predictable trajectories**, while others introduce greater complexity and instability.  \n",
    "\n",
    "2. **Error and Loss Growth in Complex Cases**  \n",
    "   - The steady increase in error and loss for **Initial Condition 2** suggests that **m2's trajectory is more chaotic** in this setting.  \n",
    "   - The LSTM struggles with **long-term dependencies**, leading to **compounded errors over time**.  \n",
    "\n",
    "3. **Potential Overfitting to Certain Trajectories**  \n",
    "   - The model may have **learned periodic behaviors** in Initial Condition 1, leading to **low error and loss** for that case.  \n",
    "   - However, it **fails to generalize** well for Initial Condition 2, where predictions degrade faster.  \n",
    "\n",
    "## **Final Conclusion**  \n",
    "- The LSTM model effectively predicts **short-term** trajectories but struggles with **long-term stability**, especially for more chaotic initial conditions.  \n",
    "- The model's accuracy **depends on the chosen initial conditions**, suggesting that training on a **wider variety of cases** could improve generalization.  \n",
    "- **Future Improvements:**\n",
    "  - Use **sequence-to-sequence architectures** to improve long-term stability.  \n",
    "  - Introduce **attention mechanisms** to focus on important patterns.  \n",
    "  - Train with **a larger dataset including more diverse initial conditions**.  \n",
    "  - Apply **physics-based constraints** to enforce physically valid predictions.  \n",
    "\n",
    "Overall, while the model works well in specific scenarios, it requires **further refinement** to handle complex dynamics over extended time horizons.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
